・计算TF-IDF值：
	from sklearn.feature_extration.text import TfidfVectorizer
	tfidf_vec = TfidfVectorizer()
	document=[...]
	matrix = tfidf_vec.fit_transform(document)拟合模型，得到矩阵
・矩阵相关信息：
	tfidf_vec.get_feature_names()输出文档所有不重复的单词
	tdidf_vec.vocabulary_ 输出每个单词对应的id
	matrix.toarray()输出每个单词在每个文档的TF-IDF值

・分词工具：
	对于English分词：
		import ntlk
		word_list = ntlk.wordtokenize(text)
		nltk.pos_tag(word_list)
	对于中文分词：
		import jieba
		word_list = jieba.cut(text)

・计算单词的权重:
	from sklearn.feature_extration.text import TfidfVectorizer
	tf = TfidfVectorizer(stop_words=stop_words,max_df=0.5)#0.5代表单词在50%出现将不作分词讨论
	features = tf.fit_transform(train_contents)

・生成朴素贝叶斯分类器：
	from sklearn.naive_bayes import MultinomialNB
	clf = MultinomialNB(alpha=0.001).fit(train_features,train_labels)
	alpha平滑在0~1 alpha越小说明迭代越深，精度越大
	
・利用生成器作预测：
	通过测试集得到测试特征矩阵：
	test_tf = TfidfVectorizer(stop_words=stop_words,max_df=0.5,vocabulary=train_vocabulary)
	test_features = test_tf.fit_transform(test_contents)
	预测：
	predicted_labels = clf.predict(test_labels)
・计算准确率：
	from sklearn import metrics
	metrics.accuracy_score(test_labels,predict_labels)

ps：以上出现就是朴素贝叶斯分类器的使用
	这个出现的几个新知识点：
	jieba.cut(content)这个分词器，分出来的是generator（生成器），使用时候需要利用list()转换
	os.walk(base_path)这个函数返回root,dir,file这个3个东西：
		root:当前遍历的地址
		dir:返回的是目录下的所有目录文件列表
		file:返回的是目录下的文件