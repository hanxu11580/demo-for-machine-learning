・贝叶斯原理：
	解决逆向概率的问题
	1、先验概率：利用经验判断事情发生的概率如：根据以往气候判断这个月下雨的概率 
	2、后验概率：通过特征症值条件下求类别的概率
	3、条件概率：通过类别条件下的概率求特征值的概率
	本质上就是计算后验概率：
		后验概率本质上属性概率论的条件概率，贝叶斯最终求的就是后验概率。
		为什么说是逆向概率呢，我们可以通过以往的经验知道先验概率，P(B|A)在先验条件A的条件下，求出B发生的概率，很自然的P(B)的概率也能求出来
	
	朴素贝叶斯分类器的工作原理：
	朴素贝叶斯认为特征属性是相互独立的。
	1、确定特征属性如 C1/C2代表男女 A1/A2/A3代表身高体重鞋码
	2、确定每一个类别下每一个特征属性的概率 求P(A1/C1)/P(A2/C1)/P(A3/C1)/C2同样计算
	3、通过计算特征属性下的类别概率：也就是计算P(C1|A1A2A3)/P(C2|A1A2A3)取最大的，从而获得预测结果

	最适用的地方是：文本分类、感情分析、垃圾邮件识别，也很适用于NLP


・在sklearn中提供了3种供我们使用：
	1、高斯朴素贝叶斯：适合连续值，且符合高斯分布（正态分布）如身高体重
	2、多项式朴素贝叶斯：适合离散变量，在文档分类中特征变量体现在一个单词出现的次数或TF-IDF值
	3、伯努利朴素贝叶斯：适合布尔变量，在文档分类中特征变量体现在一个单词是否出现
	2、3一般多用于文本分类
	
	TF-IDF=TF*IDF
	TF：词频，一个词出现在文档数
	IDF：一个词在几个文档中出现过	
	IDF=log(文档总数/(出现文档数+1))ps:+1是为了应对一个词出现在文档中的文档数为0
	如果TF_IDF值比较大，那么表示这个词作区分比较好
	